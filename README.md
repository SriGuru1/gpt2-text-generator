
---

## 💻 How It Works

1. Load GPT-2 pre-trained model and tokenizer from HuggingFace
2. Tokenize user input (prompt)
3. Generate output text using `generate()` method with `top_k` sampling
4. Decode and display the generated paragraph

---

## 🧰 Tools & Technologies

- Python 3.11+
- HuggingFace Transformers (`gpt2`)
- PyTorch
- Google Colab

---

## 🚀 Why This Project?

This project demonstrates:
- Hands-on understanding of **language modeling**
- Use of **transformer-based generative models**
- Practical skill in using HuggingFace and managing model inference

It reflects my interest and self-driven learning in **Generative AI**, one of the key focus areas of the internship.

---

## 📂 Status

✔️ Completed and working  
📁 Uploaded to GitHub for review  


---

## 📧 Contact

**Sri Guru H G**  
BE, RNS Institute of Technology  
Email: hgsriguru@gmail.com  


---

