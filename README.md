# 🧠 GPT-2 Text Generator

A simple text generator using the GPT-2 language model, built with Hugging Face Transformers. This project showcases the power of Generative AI to produce human-like text from a given prompt.

## 📦 Requirements

- Python 3.8+
- Transformers
- Torch
- Colab or Jupyter Notebook (Recommended)

## 🚀 How to Run

1. Open the notebook in Google Colab.
2. Install required libraries (auto-install in Colab).
3. Run all cells and enter a prompt to generate text.

## 🧪 Sample Output

Input Prompt: "The future of AI in India"

Generated Output: "The future of AI in India is promising. With increasing investments, a surge in talent, and government initiatives, the AI ecosystem is poised to revolutionize industries across the country."


> Note: Output may vary slightly each time due to the probabilistic nature of GPT-2.

## 📁 Dataset & Model

- Uses pre-trained GPT-2 (`distilgpt2`) from Hugging Face.
- No external training required.

## 📚 Domain Relevance

This project aligns with the **Generative AI** and **Explainable AI** domain offered in the IIT Kharagpur Summer Internship 2025. It highlights key skills in:
- NLP
- Transformers
- Hugging Face
- Python

