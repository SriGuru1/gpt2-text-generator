
---

## ğŸ’» How It Works

1. Load GPT-2 pre-trained model and tokenizer from HuggingFace
2. Tokenize user input (prompt)
3. Generate output text using `generate()` method with `top_k` sampling
4. Decode and display the generated paragraph

---

## ğŸ§° Tools & Technologies

- Python 3.11+
- HuggingFace Transformers (`gpt2`)
- PyTorch
- Google Colab

---

## ğŸš€ Why This Project?

This project demonstrates:
- Hands-on understanding of **language modeling**
- Use of **transformer-based generative models**
- Practical skill in using HuggingFace and managing model inference

It reflects my interest and self-driven learning in **Generative AI**, one of the key focus areas of the internship.

---

## ğŸ“‚ Status

âœ”ï¸ Completed and working  
ğŸ“ Uploaded to GitHub for review  


---

## ğŸ“§ Contact

**Sri Guru H G**  
BE, RNS Institute of Technology  
Email: hgsriguru@gmail.com  


---

