# ğŸ§  GPT-2 Text Generator

A simple text generator using the GPT-2 language model, built with Hugging Face Transformers. This project showcases the power of Generative AI to produce human-like text from a given prompt.

## ğŸ“¦ Requirements

- Python 3.8+
- Transformers
- Torch
- Colab or Jupyter Notebook (Recommended)

## ğŸš€ How to Run

1. Open the notebook in Google Colab.
2. Install required libraries (auto-install in Colab).
3. Run all cells and enter a prompt to generate text.

## ğŸ§ª Sample Output

Input Prompt: "The future of AI in India"

Generated Output: "The future of AI in India is promising. With increasing investments, a surge in talent, and government initiatives, the AI ecosystem is poised to revolutionize industries across the country."


> Note: Output may vary slightly each time due to the probabilistic nature of GPT-2.

## ğŸ“ Dataset & Model

- Uses pre-trained GPT-2 (`distilgpt2`) from Hugging Face.
- No external training required.

## ğŸ“š Domain Relevance

This project aligns with the **Generative AI** and **Explainable AI** domain offered in the IIT Kharagpur Summer Internship 2025. It highlights key skills in:
- NLP
- Transformers
- Hugging Face
- Python

